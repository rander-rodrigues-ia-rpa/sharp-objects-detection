# Sharp Objects Detection

Projeto de detecÃ§Ã£o de objetos cortantes em vÃ­deos usando YOLOv8 com treinamento personalizado e API REST para envio de vÃ­deos e alertas.

---

## ğŸ§  Funcionalidades

- Treinamento de modelo YOLO com dataset customizado
- Upload de vÃ­deos para anÃ¡lise via API
- GeraÃ§Ã£o de vÃ­deo com detecÃ§Ã£o de objetos
- Envio de alerta por e-mail ao detectar objeto cortante
- Interface de API interativa via Swagger (FastAPI)

---

## ğŸ“¦ Estrutura do Projeto

```
Sharp_objects_detection/
â”œâ”€â”€ api/                  # Endpoints da API
â”‚   â””â”€â”€ frontend.py
|   |__ main.py
â”œâ”€â”€ app/                  # LÃ³gica do modelo e serviÃ§os
|   |__ alertatelegram.py
â”‚   â”œâ”€â”€ detector.py
â”‚   â”œâ”€â”€ trainer.py
â”‚   â””â”€â”€ email_alert.py
|   |__ trainer.py
â”œâ”€â”€ models/               # Modelos treinados (best.pt)
â”‚   â””â”€â”€ objeto_cortante.pt
â”œâ”€â”€ videos/
â”‚   â”œâ”€â”€ input/            # VÃ­deos enviados para anÃ¡lise
â”‚   â””â”€â”€ output/           # VÃ­deos processados com detecÃ§Ã£o
â”œâ”€â”€ config/               # Arquivo YAML do dataset
â”‚   â””â”€â”€ data.yaml
â”œâ”€â”€ requirements.txt      # DependÃªncias do projeto
â””â”€â”€ README.md             # Este arquivo
```

---

## ğŸ› ï¸ Como Iniciar o Projeto Localmente

### 1. Baixando o Projeto
Para comeÃ§ar, clone o repositÃ³rio do projeto no seu ambiente local usando o Git:
```bash
git clone https://github.com/rander-rodrigues-ia-rpa/sharp-objects-detection.git
```

### 2. Configurando o Ambiente Local
## 2.1 Criar o Ambiente Virtual
Primeiro, crie um ambiente virtual para isolar as dependÃªncias do projeto.
```bash
python -m venv environment
```

## 2.2 Ativar o Ambiente Virtual
Ative o ambiente virtual com o seguinte comando:
Windows:
```bash
.\environment\Scripts\Activate
```

Linux/macOS:
```bash
source environment/bin/activate
```

## 2.3 Instalar as DependÃªncias
Instale todas as dependÃªncias necessÃ¡rias para o projeto:
```bash
pip install -r requirements.txt
```

### 3. Rodando a API Backend (FastAPI)
A API foi construÃ­da usando FastAPI. Para rodÃ¡-la, siga os seguintes passos:

## 3.1 Entrar na pasta do projeto
Se vocÃª ainda nÃ£o estiver na pasta principal do projeto, entre nela:
```bash
cd SHARP-OBJECTS-DETECTION
```

## 3.2 Iniciar o Backend
Inicie o servidor backend com o seguinte comando:
```bash
uvicorn api.main:app --reload
```
O servidor backend estarÃ¡ disponÃ­vel em http://127.0.0.1:8000.

### 4. Rodando o Frontend (Streamlit)
Agora, vamos rodar o frontend com Streamlit:

## 4.1 Entrar na Pasta api
Se vocÃª nÃ£o estÃ¡ na pasta api, vÃ¡ atÃ© ela:
```bash
cd api
```
## 4.2 Iniciar o Frontend
Agora, execute o Streamlit para rodar o frontend da aplicaÃ§Ã£o:
```bash
streamlit run frontend.py
```
O frontend serÃ¡ acessado em http://127.0.0.1:8501.

âš ï¸ ObservaÃ§Ãµes Importantes:
Backend (API): A API deve ser iniciada com o comando uvicorn antes de rodar o frontend.

Frontend (Streamlit): Certifique-se de entrar na pasta api antes de rodar o frontend, jÃ¡ que o frontend.py estÃ¡ localizado lÃ¡.


### ğŸ“¤ Treinamento do Modelo
Este mÃ©todo permite treinar o modelo via API
## Endpoint: `POST /treinar-modelo`
```bash
curl --location 'http://localhost:8000/treinar-modelo' \
--form 'dataset_path="Cortantes.v1i.yolov12"' \
--form 'epochs="100"'
```

**Campos:**
- `dataset_path`: Caminho da pasta do dataset (ex: `Cortantes.v1i.yolov12`)
- `epochs` (opcional): NÃºmero de Ã©pocas de treinamento

```json
{
  "dataset_path": "Cortantes.v1i.yolov12",
  "epochs": 100
}
```
O modelo treinado serÃ¡ salvo automaticamente em `models/objeto_cortante.pt`
### Importante: O modelo inicial do YOLO deve estar presente em uma pasta na raiz do projeto com o seguinte nome: Cortantes.v1i.yolov12 
---

## ğŸ“¼ AnÃ¡lise de VÃ­deo

### Endpoint: `POST /analisar-video`
```bash
curl --location 'http://127.0.0.1:8000/analisar-video' \
--header 'accept: application/json' \
--form 'video=@"/C:/Users/sharp-objects-detection/videos/input/video2.mp4"' \
--form 'alertar_telegram="False"' \
--form 'usuario_telegram="rrr"' \
--form 'gerar_video="true"' \
--form 'alertar_email="False"' \
--form 'destinatario_email="xxx@gmail.com"'
```

### Endpoint: `POST /registrar-telegram`
```bash
curl --location 'http://127.0.0.1:8000/registrar-telegram' \
--header 'accept: application/json' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data-urlencode 'usuario_telegram=rrr'
```
Importante: Este endpoint deve ser consumido somente apÃ³s a interaÃ§Ã£o com o bot no telegram. 
Abra seu Telegram e encontre o seguinte usuÃ¡rio: sharpobjectdetectionBot. Diga "OlÃ¡" para o sharpobjectdetectionBot iniciar uma conversa com vocÃª. para conferir se o usuÃ¡rio foi registrado no chat, basta acionar a api do Telegram informando o Token da conversa com o Bot: https://api.telegram.org/SEU-TOKEN-BOT/getUpdates


## ğŸ”§ Requisitos
- Python 3.8+
- `yolov8n.pt` (modelo base da ultralytics)
- Dataset estruturado com `train/images`, `train/labels`, `valid/images`, `valid/labels`

### Manual do UsuÃ¡rio
```bash
1 - Passo 1: FaÃ§a o upload do vÃ­deo que deseja ser analisado.
2 - Passo 2: Escolha o mÃ©todo de alerta.
  As opÃ§Ãµes de alerta disponÃ­veis sÃ£o:
     2.1 - Telegram: Informe o nome de usuÃ¡rio o Telegram, interaja com o Bot no telegram (sharpobjectdetectionBot)
     2.2 - E-mail: Informe o e-mail do destinatÃ¡rio para receber os alertas via e-mail.
     2.3 - Apenas gerar vÃ­deo: Essa opÃ§Ã£o permite fazer o download do vÃ­deo analisado.
3 - Escolha o limiar de confianÃ§a para a detecÃ§Ã£o de imagens.
```
